{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graded Assignment-4 torch VGG16.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNxYtfpkFgcdZegWeqa7gIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrnTEK/Deep-Learning/blob/master/Graded_Assignment_4_torch_VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZuSgYIm6JWq",
        "colab_type": "text"
      },
      "source": [
        "##Step-1 Loading Libraries as required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfpwmU8W6A7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTulJxKo6TDA",
        "colab_type": "text"
      },
      "source": [
        "##Step-2 VGG16 model Function  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGsNwy856OT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # fully conected layers:\n",
        "        self.fc6 = nn.Linear(2048, 4096)\n",
        "        self.fc7 = nn.Linear(4096, 4096)\n",
        "        self.fc8 = nn.Linear(4096, 1000)\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        x = F.relu(self.conv1_1(x))\n",
        "        x = F.relu(self.conv1_2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2_1(x))\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3_1(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        x = F.relu(self.conv3_3(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv4_1(x))\n",
        "        x = F.relu(self.conv4_2(x))\n",
        "        x = F.relu(self.conv4_3(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1) \n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = F.dropout(x, 0.5, training=training)\n",
        "        x = F.relu(self.fc7(x))\n",
        "        x = F.dropout(x, 0.5, training=training)\n",
        "        x = self.fc8(x)\n",
        "        return x\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUzeyH-W6ck5",
        "colab_type": "text"
      },
      "source": [
        "##Step-3 Function for Loading data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ebgb6N6YJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(train_batch_size, test_batch_size):\n",
        "    # Fetch training data: total 60000 samples\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR100('data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize((32, 32)),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "    # Fetch test data: total 10000 samples\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR100('data', train=False, transform=transforms.Compose([\n",
        "            transforms.Resize((32, 32)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])),\n",
        "        batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "    return (train_loader, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h12jgIcs6z6e",
        "colab_type": "text"
      },
      "source": [
        "##Step-4 Function for training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTNNK0Uv6hPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, epoch, train_loader, log_interval):\n",
        "    model.train()\n",
        "\n",
        "    # define loss function\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Iterate over batches of data\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Wrap the input and target output in the `Variable` wrapper\n",
        "        data,target = data.to(device),target.to(device)\n",
        "        data, target = Variable(data), Variable(target)\n",
        "\n",
        "        # Clear the gradients, since PyTorch accumulates them\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        output = model(data)\n",
        "\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters(weight,bias)\n",
        "        optimizer.step()\n",
        "\n",
        "        # print log\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train set, Epoch {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                       100. * batch_idx / len(train_loader),\n",
        "                loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6fJ5XBs67-N",
        "colab_type": "text"
      },
      "source": [
        "##Step-5 Function for Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WL7Sw_367Ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, epoch, test_loader):\n",
        "    # State that you are testing the model; this prevents layers e.g. Dropout to take effect\n",
        "    model.eval()\n",
        "\n",
        "    # Init loss & correct prediction accumulators\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # define loss function\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(size_average=False)\n",
        "\n",
        "    # Iterate over data\n",
        "    for data, target in test_loader:\n",
        "        data,target = data.to(device),target.to(device)\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        \n",
        "        # Forward propagation\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate & accumulate loss\n",
        "        test_loss += loss_fn(output, target).item()\n",
        "\n",
        "        # Get the index of the max log-probability (the predicted output label)\n",
        "        pred = np.argmax(output.cpu().data, axis=1)\n",
        "\n",
        "        # If correct, increment correct prediction accumulator\n",
        "        correct = correct + np.equal(pred, target.cpu().data).sum()\n",
        "\n",
        "    # Print log\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set, Epoch {} , Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(epoch,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuB5caU-7CzU",
        "colab_type": "text"
      },
      "source": [
        "##Step-6 Modelling and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynXhbgJa7B3_",
        "colab_type": "code",
        "outputId": "20a266ae-8495-4a92-e1a3-21dc3c723825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "model = VGG16()\n",
        "model.cuda()\n",
        "lr = 0.01\n",
        "momentum=0.5\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "train_batch_size = 64\n",
        "test_batch_size = 1000\n",
        "train_loader, test_loader = load_data(train_batch_size, test_batch_size)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJyrrF3v7It2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97b33419-2fcc-469b-fa6f-405411d2920c"
      },
      "source": [
        "epochs = 10\n",
        "log_interval = 100\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, optimizer, epoch, train_loader, log_interval=log_interval)\n",
        "    test(model, epoch, test_loader)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set, Epoch 1 [0/50000 (0%)]\tLoss: 6.908016\n",
            "Train set, Epoch 1 [6400/50000 (13%)]\tLoss: 6.881804\n",
            "Train set, Epoch 1 [12800/50000 (26%)]\tLoss: 6.853255\n",
            "Train set, Epoch 1 [19200/50000 (38%)]\tLoss: 6.818030\n",
            "Train set, Epoch 1 [25600/50000 (51%)]\tLoss: 6.772709\n",
            "Train set, Epoch 1 [32000/50000 (64%)]\tLoss: 6.702904\n",
            "Train set, Epoch 1 [38400/50000 (77%)]\tLoss: 6.500972\n",
            "Train set, Epoch 1 [44800/50000 (90%)]\tLoss: 4.717279\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set, Epoch 1 , Average loss: 4.7541, Accuracy: 96/10000 (1%)\n",
            "\n",
            "Train set, Epoch 2 [0/50000 (0%)]\tLoss: 4.785915\n",
            "Train set, Epoch 2 [6400/50000 (13%)]\tLoss: 4.668557\n",
            "Train set, Epoch 2 [12800/50000 (26%)]\tLoss: 4.676157\n",
            "Train set, Epoch 2 [19200/50000 (38%)]\tLoss: 4.696191\n",
            "Train set, Epoch 2 [25600/50000 (51%)]\tLoss: 4.662859\n",
            "Train set, Epoch 2 [32000/50000 (64%)]\tLoss: 4.701272\n",
            "Train set, Epoch 2 [38400/50000 (77%)]\tLoss: 4.640639\n",
            "Train set, Epoch 2 [44800/50000 (90%)]\tLoss: 4.707325\n",
            "\n",
            "Test set, Epoch 2 , Average loss: 4.6837, Accuracy: 104/10000 (1%)\n",
            "\n",
            "Train set, Epoch 3 [0/50000 (0%)]\tLoss: 4.611873\n",
            "Train set, Epoch 3 [6400/50000 (13%)]\tLoss: 4.654790\n",
            "Train set, Epoch 3 [12800/50000 (26%)]\tLoss: 4.655738\n",
            "Train set, Epoch 3 [19200/50000 (38%)]\tLoss: 4.685040\n",
            "Train set, Epoch 3 [25600/50000 (51%)]\tLoss: 4.711762\n",
            "Train set, Epoch 3 [32000/50000 (64%)]\tLoss: 4.649308\n",
            "Train set, Epoch 3 [38400/50000 (77%)]\tLoss: 4.705639\n",
            "Train set, Epoch 3 [44800/50000 (90%)]\tLoss: 4.684082\n",
            "\n",
            "Test set, Epoch 3 , Average loss: 4.6689, Accuracy: 104/10000 (1%)\n",
            "\n",
            "Train set, Epoch 4 [0/50000 (0%)]\tLoss: 4.686057\n",
            "Train set, Epoch 4 [6400/50000 (13%)]\tLoss: 4.680564\n",
            "Train set, Epoch 4 [12800/50000 (26%)]\tLoss: 4.652026\n",
            "Train set, Epoch 4 [19200/50000 (38%)]\tLoss: 4.710889\n",
            "Train set, Epoch 4 [25600/50000 (51%)]\tLoss: 4.690709\n",
            "Train set, Epoch 4 [32000/50000 (64%)]\tLoss: 4.640155\n",
            "Train set, Epoch 4 [38400/50000 (77%)]\tLoss: 4.616602\n",
            "Train set, Epoch 4 [44800/50000 (90%)]\tLoss: 4.625233\n",
            "\n",
            "Test set, Epoch 4 , Average loss: 4.6599, Accuracy: 103/10000 (1%)\n",
            "\n",
            "Train set, Epoch 5 [0/50000 (0%)]\tLoss: 4.719212\n",
            "Train set, Epoch 5 [6400/50000 (13%)]\tLoss: 4.602951\n",
            "Train set, Epoch 5 [12800/50000 (26%)]\tLoss: 4.658518\n",
            "Train set, Epoch 5 [19200/50000 (38%)]\tLoss: 4.637560\n",
            "Train set, Epoch 5 [25600/50000 (51%)]\tLoss: 4.636601\n",
            "Train set, Epoch 5 [32000/50000 (64%)]\tLoss: 4.610967\n",
            "Train set, Epoch 5 [38400/50000 (77%)]\tLoss: 4.649738\n",
            "Train set, Epoch 5 [44800/50000 (90%)]\tLoss: 4.682204\n",
            "\n",
            "Test set, Epoch 5 , Average loss: 4.6610, Accuracy: 99/10000 (1%)\n",
            "\n",
            "Train set, Epoch 6 [0/50000 (0%)]\tLoss: 4.718044\n",
            "Train set, Epoch 6 [6400/50000 (13%)]\tLoss: 4.685931\n",
            "Train set, Epoch 6 [12800/50000 (26%)]\tLoss: 4.644931\n",
            "Train set, Epoch 6 [19200/50000 (38%)]\tLoss: 4.673845\n",
            "Train set, Epoch 6 [25600/50000 (51%)]\tLoss: 4.661873\n",
            "Train set, Epoch 6 [32000/50000 (64%)]\tLoss: 4.609734\n",
            "Train set, Epoch 6 [38400/50000 (77%)]\tLoss: 4.699080\n",
            "Train set, Epoch 6 [44800/50000 (90%)]\tLoss: 4.649420\n",
            "\n",
            "Test set, Epoch 6 , Average loss: 4.6542, Accuracy: 89/10000 (1%)\n",
            "\n",
            "Train set, Epoch 7 [0/50000 (0%)]\tLoss: 4.637411\n",
            "Train set, Epoch 7 [6400/50000 (13%)]\tLoss: 4.689642\n",
            "Train set, Epoch 7 [12800/50000 (26%)]\tLoss: 4.678153\n",
            "Train set, Epoch 7 [19200/50000 (38%)]\tLoss: 4.643690\n",
            "Train set, Epoch 7 [25600/50000 (51%)]\tLoss: 4.637938\n",
            "Train set, Epoch 7 [32000/50000 (64%)]\tLoss: 4.613090\n",
            "Train set, Epoch 7 [38400/50000 (77%)]\tLoss: 4.681410\n",
            "Train set, Epoch 7 [44800/50000 (90%)]\tLoss: 4.659170\n",
            "\n",
            "Test set, Epoch 7 , Average loss: 4.6443, Accuracy: 98/10000 (1%)\n",
            "\n",
            "Train set, Epoch 8 [0/50000 (0%)]\tLoss: 4.651517\n",
            "Train set, Epoch 8 [6400/50000 (13%)]\tLoss: 4.601042\n",
            "Train set, Epoch 8 [12800/50000 (26%)]\tLoss: 4.594771\n",
            "Train set, Epoch 8 [19200/50000 (38%)]\tLoss: 4.667577\n",
            "Train set, Epoch 8 [25600/50000 (51%)]\tLoss: 4.694071\n",
            "Train set, Epoch 8 [32000/50000 (64%)]\tLoss: 4.674176\n",
            "Train set, Epoch 8 [38400/50000 (77%)]\tLoss: 4.648258\n",
            "Train set, Epoch 8 [44800/50000 (90%)]\tLoss: 4.635550\n",
            "\n",
            "Test set, Epoch 8 , Average loss: 4.6433, Accuracy: 87/10000 (1%)\n",
            "\n",
            "Train set, Epoch 9 [0/50000 (0%)]\tLoss: 4.615564\n",
            "Train set, Epoch 9 [6400/50000 (13%)]\tLoss: 4.658188\n",
            "Train set, Epoch 9 [12800/50000 (26%)]\tLoss: 4.623253\n",
            "Train set, Epoch 9 [19200/50000 (38%)]\tLoss: 4.619349\n",
            "Train set, Epoch 9 [25600/50000 (51%)]\tLoss: 4.693107\n",
            "Train set, Epoch 9 [32000/50000 (64%)]\tLoss: 4.656072\n",
            "Train set, Epoch 9 [38400/50000 (77%)]\tLoss: 4.660141\n",
            "Train set, Epoch 9 [44800/50000 (90%)]\tLoss: 4.665773\n",
            "\n",
            "Test set, Epoch 9 , Average loss: 4.6431, Accuracy: 94/10000 (1%)\n",
            "\n",
            "Train set, Epoch 10 [0/50000 (0%)]\tLoss: 4.623075\n",
            "Train set, Epoch 10 [6400/50000 (13%)]\tLoss: 4.628254\n",
            "Train set, Epoch 10 [12800/50000 (26%)]\tLoss: 4.615532\n",
            "Train set, Epoch 10 [19200/50000 (38%)]\tLoss: 4.688163\n",
            "Train set, Epoch 10 [25600/50000 (51%)]\tLoss: 4.690535\n",
            "Train set, Epoch 10 [32000/50000 (64%)]\tLoss: 4.618001\n",
            "Train set, Epoch 10 [38400/50000 (77%)]\tLoss: 4.674712\n",
            "Train set, Epoch 10 [44800/50000 (90%)]\tLoss: 4.619642\n",
            "\n",
            "Test set, Epoch 10 , Average loss: 4.6420, Accuracy: 92/10000 (1%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2JJMp6ht0Hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}