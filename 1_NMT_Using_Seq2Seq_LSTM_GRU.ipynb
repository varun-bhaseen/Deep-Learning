{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1_NMT_Using_Seq2Seq_LSTM_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrnTEK/Deep-Learning/blob/master/1_NMT_Using_Seq2Seq_LSTM_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# NMT with Seq2Seq, LSTM, and GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "import numpy as np\n",
        "import unicodedata, re,os, io, time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Download  the dataset\n",
        "\n",
        "Use language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y87-Pn39qLL7",
        "colab_type": "text"
      },
      "source": [
        "### Download the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kRVATYOgJs1b",
        "colab": {}
      },
      "source": [
        "path = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip'\n",
        "\n",
        "file_zip = keras.utils.get_file('spa-eng.zip', origin=path, extract=True)\n",
        "\n",
        "file_path = os.path.dirname(file_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMTbPUzHqEBI",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprcocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-saqy9z9oPYY",
        "colab_type": "text"
      },
      "source": [
        "#### Converts the unicode file to ascii"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rd0jw-eC3jEh",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "def file_to_ascii(u):\n",
        "  return ''.join(f for f in unicodedata.normalize('NFD', u)\n",
        "      if unicodedata.category(f) != 'Mn')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owc8IKmyoNwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = file_to_ascii(w.lower().strip())\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "opI2GzOt479E",
        "outputId": "205720ef-fa97-4fd4-e7ee-f3e830d531a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#test\n",
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OHn4Dct23jEm",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cTbSbBz55QtF",
        "outputId": "d220c232-6a2a-4a28-ef0c-2f1c93d067fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "en, sp = create_dataset(file_path, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIOn8RCNDJXG",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eAY9k49G3jE_",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GOi42V79Ydlr"
      },
      "source": [
        "### Limit the size of the dataset to experiment faster (optional)\n",
        "\n",
        "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnxC7q-j3jFD",
        "colab": {}
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(file_path, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QILQkOs3jFG",
        "outputId": "370177ed-65d3-4b1e-e28c-5df324046cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = tts(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24000 24000 6000 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lJPmLZGMeD5q",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VXukARTDd7MT",
        "outputId": "4dbf5ba1-9c17-4346-9fdf-1d597be6a2aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "165 ----> ven\n",
            "10 ----> a\n",
            "39 ----> las\n",
            "98 ----> dos\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "53 ----> come\n",
            "55 ----> at\n",
            "208 ----> two\n",
            "1437 ----> o\n",
            "654 ----> clock\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqHsArVZ3jFS",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHBoXsCCKpKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qc6-NK1GtWQt",
        "outputId": "1db37c92-4b24-4d87-ee95-139d4db30782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## Write the encoder and decoder model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60gSVh05Jl6l",
        "outputId": "5bb5dafc-f831-404b-d827-0f4cdbaba146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umohpBN2OM94",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k534zTHiDjQU",
        "outputId": "859ba154-ec85-4799-f764-642f95a8cf0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5UY8wko3jFp",
        "outputId": "a75a671f-605c-4435-9b2e-2592e835b7b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpObfY22IddU"
      },
      "source": [
        "## Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC9ArXSsVfqn",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ddefjBMa3jF0",
        "outputId": "8e1b0ebe-4756-48f5-8965-b42074c2eef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "# EPOCHS = 1\n",
        "# EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.7113\n",
            "Epoch 1 Batch 100 Loss 2.2246\n",
            "Epoch 1 Batch 200 Loss 1.8710\n",
            "Epoch 1 Batch 300 Loss 1.6476\n",
            "Epoch 1 Loss 2.0142\n",
            "Time taken for 1 epoch 87.34355187416077 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.4443\n",
            "Epoch 2 Batch 100 Loss 1.4125\n",
            "Epoch 2 Batch 200 Loss 1.3707\n",
            "Epoch 2 Batch 300 Loss 1.2389\n",
            "Epoch 2 Loss 1.3808\n",
            "Time taken for 1 epoch 87.01994228363037 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.0441\n",
            "Epoch 3 Batch 100 Loss 1.0468\n",
            "Epoch 3 Batch 200 Loss 1.0491\n",
            "Epoch 3 Batch 300 Loss 0.8610\n",
            "Epoch 3 Loss 0.9773\n",
            "Time taken for 1 epoch 87.5256507396698 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.6911\n",
            "Epoch 4 Batch 100 Loss 0.6269\n",
            "Epoch 4 Batch 200 Loss 0.6168\n",
            "Epoch 4 Batch 300 Loss 0.5835\n",
            "Epoch 4 Loss 0.6529\n",
            "Time taken for 1 epoch 87.6522102355957 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.3881\n",
            "Epoch 5 Batch 100 Loss 0.4822\n",
            "Epoch 5 Batch 200 Loss 0.4651\n",
            "Epoch 5 Batch 300 Loss 0.4387\n",
            "Epoch 5 Loss 0.4322\n",
            "Time taken for 1 epoch 86.95227646827698 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.2345\n",
            "Epoch 6 Batch 100 Loss 0.3171\n",
            "Epoch 6 Batch 200 Loss 0.2461\n",
            "Epoch 6 Batch 300 Loss 0.2863\n",
            "Epoch 6 Loss 0.2898\n",
            "Time taken for 1 epoch 88.223562002182 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2135\n",
            "Epoch 7 Batch 100 Loss 0.1726\n",
            "Epoch 7 Batch 200 Loss 0.1910\n",
            "Epoch 7 Batch 300 Loss 0.1854\n",
            "Epoch 7 Loss 0.1988\n",
            "Time taken for 1 epoch 87.31602144241333 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1601\n",
            "Epoch 8 Batch 100 Loss 0.1128\n",
            "Epoch 8 Batch 200 Loss 0.1529\n",
            "Epoch 8 Batch 300 Loss 0.1403\n",
            "Epoch 8 Loss 0.1437\n",
            "Time taken for 1 epoch 87.84719228744507 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0745\n",
            "Epoch 9 Batch 100 Loss 0.0694\n",
            "Epoch 9 Batch 200 Loss 0.0894\n",
            "Epoch 9 Batch 300 Loss 0.0886\n",
            "Epoch 9 Loss 0.1119\n",
            "Time taken for 1 epoch 87.81808924674988 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1086\n",
            "Epoch 10 Batch 100 Loss 0.0918\n",
            "Epoch 10 Batch 200 Loss 0.1546\n",
            "Epoch 10 Batch 300 Loss 0.0918\n",
            "Epoch 10 Loss 0.0905\n",
            "Time taken for 1 epoch 88.69482278823853 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbQpyYs13jF_",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5hQWlbN3jGF",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sl9zUHzg3jGI",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJpT9D5_OgP6",
        "outputId": "440af78d-195e-4d6c-9b6c-7c6724ee5610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6422899710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A3LLCx3ZE0Ls",
        "outputId": "890f39e5-fffe-4467-de1f-47fde06f415a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "translate(u'¿todavia estan en casa?')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> ¿ todavia estan en casa ? <end>\n",
            "Predicted translation: is still still at home ? <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DUQVLVqUE1YW",
        "outputId": "c707f66c-65e9-437d-93ad-2371806a7ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        }
      },
      "source": [
        "translate(u'trata de averiguarlo.')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> trata de averiguarlo . <end>\n",
            "Predicted translation: try to figure it out . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cz_baTULCCE",
        "colab_type": "text"
      },
      "source": [
        "This classification shows a wrong output. According to words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMFutL_oLGy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}